---
title: "Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals"
collection: publications
permalink: /publication/RnR
excerpt: 'Learning to Play Atari with the Help of Instruction Manuals'
date: 2023-02-12
venue: 'Preprint'
paperurl: 'https://arxiv.org/pdf/2302.04449.pdf'
authors: 'Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, Tom M. Mitchell'
img: 'https://www.yuewu.ml/files/demos/RnR_1.gif'
submitted: false
# citation: 'Your Name, You. (2015). &quot;Paper Title Number 3.&quot; <i>Journal 1</i>. 1(3).'
---
[Full Text](https://arxiv.org/pdf/2010.14543.pdf), [Press Coverage](https://www.newscientist.com/article/2358953-ai-masters-video-game-6000-times-faster-by-reading-the-instructions/)

### Abstract
High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent.
We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. Auxiliary reward is then provided to a standard A2C RL agent, when interaction is detected. When assisted by our design, A2C improves on 4 games in the Atari environment with sparse rewards, and requires 1000x less training frames compared to the previous SOTA Agent 57 on Skiing, the hardest game in Atari.

## Walkthrough
The trial-and-error process of RL is known to be inefficient. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals or Wiki pages.

We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading Wikipedia articles and manuals released by the Atari game developers.
<img src="https://www.yuewu.ml/files/demos/RnR_1.gif"
     alt="Full Read and Reward Framework"/>

The QA Extraction module (Read) extracts and summarizes relevant information from the manual.
<img src="https://www.yuewu.ml/files/demos/RnR_2.gif"
     alt="Read"/>

The Reasoning module (reward), powered by LLM, evaluates object-agent interactions based on information from the manual.
<img src="https://www.yuewu.ml/files/demos/RnR_3.gif"
     alt="Reward"/>

Assisted by the auxiliary rewards, a standard A2C agent (takes greyscale image as input and outputs action) achieves competitive performance while using significantly less training frames compared to the SOTA on Skiing, one of the hardest Atari games for RL.
<img src="https://www.yuewu.ml/files/demos/RnR_4.png"
     alt="Scatter Plot"/>

Read and Reward can process information from various sources (Wikipedia and official manual). In addition, Read and Reward only provides auxiliary reward for the existing RL agent and does not modify the agent architecture. It could be applied to other RL algorithms.